{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df249e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook we are using Ollama to use and download open-source models like llama | mistral | etc\n",
    "# We can download and pull embedding models as well\n",
    "# I downloaded these models on my PC but it takes 15 minutes on an average to respond to user query\n",
    "# Running this on HPC reduced response time to almost 2 minutes on an average and on HPC we can use bigger models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "538475eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "!OLLAMA_ACCELERATE=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a884afe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama.pull(\"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5253d7fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.pull(\"nomic-embed-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b5bf72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import re\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from markdown import markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c685fdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "VECTOR_DB_NAME = \"local-rag\"\n",
    "local_model = \"llama3.2\"\n",
    "llm = ChatOllama(model=local_model)\n",
    "vector_db = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "333d8d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load PDF\n",
    "def load_pdf(file_path):\n",
    "    loader = PyPDFLoader(file_path=file_path)\n",
    "    return loader.load()\n",
    "\n",
    "# Function to split text\n",
    "def split_text(data, chunk_size=1000, chunk_overlap=200):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return text_splitter.split_documents(data)\n",
    "\n",
    "# Function to create vector database\n",
    "def create_vector_db(chunks):\n",
    "    global vector_db\n",
    "    if not chunks:\n",
    "        return \"Error: No text extracted from the PDF.\"\n",
    "    \n",
    "    vector_db = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=OllamaEmbeddings(model=\"nomic-embed-text\"),\n",
    "        collection_name=VECTOR_DB_NAME,\n",
    "        persist_directory=\"./chroma_db7\"\n",
    "#         persist_directory=None # Set to None for in-memory storage -> so that embeddings are not saved to disk\n",
    "    )\n",
    "    return \"PDF processed successfully! You can now ask questions.\"\n",
    "\n",
    "# Function to set up retriever\n",
    "def get_retriever():\n",
    "    if not vector_db:\n",
    "        return None\n",
    "    \n",
    "    query_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Generate 2 alternative versions of the question to improve retrieval:\n",
    "        Original: {question}\"\"\"\n",
    "    )\n",
    "    \n",
    "    return MultiQueryRetriever.from_llm(vector_db.as_retriever(), llm, prompt=query_prompt)\n",
    "\n",
    "# Function to create RAG chain\n",
    "def create_rag_chain():\n",
    "    retriever = get_retriever()\n",
    "    if not retriever:\n",
    "        return None\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"You are an AI assistant that answers questions based only on the given context.\n",
    "        Provide a well-structured, coherent, and concise response.\n",
    "\n",
    "        ### Context:\n",
    "        {context}\n",
    "\n",
    "        ### Question:\n",
    "        {question}\n",
    "\n",
    "        ### Answer:\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    return (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser()\n",
    "    )\n",
    "\n",
    "# Function to process user query\n",
    "def process_query(question, chat_history):\n",
    "    if not vector_db:\n",
    "        return \"Error: No vector database found. Please upload and process a PDF first.\", chat_history\n",
    "\n",
    "    chain = create_rag_chain()\n",
    "    if not chain:\n",
    "        return \"Error: Unable to initialize the RAG chain.\", chat_history\n",
    "\n",
    "    response = chain.invoke(question)\n",
    "    \n",
    "    # Extract only the answer from the response\n",
    "    answer_match = re.search(r\"### Answer:\\s*(.*)\", response, re.DOTALL)\n",
    "    answer = answer_match.group(1).strip() if answer_match else response.strip()\n",
    "    \n",
    "    chat_history.append((question, markdown(answer)))\n",
    "    return \"\", chat_history\n",
    "\n",
    "# Function to process PDF upload\n",
    "def process_pdf(file):\n",
    "    global vector_db\n",
    "    if not file:\n",
    "        return \"Please upload a valid PDF file.\"\n",
    "    \n",
    "    # Reset the vector DB before adding new embeddings\n",
    "    vector_db = None\n",
    "    # Load and process the PDF\n",
    "    file_path = file.name\n",
    "    data = load_pdf(file_path)\n",
    "    chunks = split_text(data)\n",
    "    \n",
    "    return create_vector_db(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c89ef36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7866\n",
      "Running on public URL: https://7cb64de9ad943f2ffb.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://7cb64de9ad943f2ffb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7866 <> https://7cb64de9ad943f2ffb.gradio.live\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradio UI\n",
    "def gradio_ui():\n",
    "    with gr.Blocks(theme=\"soft\") as demo:\n",
    "        gr.Markdown(\"<h1 style='text-align: center; color: #4A90E2;'>üìñ Conversational AI for PDFs</h1>\")\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=2):\n",
    "                chatbot = gr.Chatbot(label=\"AI Chat\")\n",
    "                user_input = gr.Textbox(placeholder=\"Ask me a question...\", label=\"Your Question\")\n",
    "\n",
    "                with gr.Row():\n",
    "                    ask_button = gr.Button(\"üîç Ask\", variant=\"primary\")\n",
    "                    clear_button = gr.Button(\"üóëÔ∏è Clear\")\n",
    "\n",
    "            with gr.Column(scale=1):\n",
    "                gr.Markdown(\"### Upload PDFs Here:\")\n",
    "                pdf_upload = gr.File(label=\"Upload PDF\", file_types=[\".pdf\"])\n",
    "                status = gr.Textbox(label=\"Status\", interactive=False)\n",
    "\n",
    "        # Define button actions\n",
    "        ask_button.click(process_query, inputs=[user_input, chatbot], outputs=[user_input, chatbot])\n",
    "        clear_button.click(lambda: [], outputs=[chatbot])\n",
    "        pdf_upload.change(process_pdf, inputs=[pdf_upload], outputs=[status])\n",
    "\n",
    "    return demo\n",
    "\n",
    "demo = gradio_ui()\n",
    "demo.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beeb9f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rag_env)",
   "language": "python",
   "name": "rag_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
