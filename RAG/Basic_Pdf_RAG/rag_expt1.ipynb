{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "!OLLAMA_ACCELERATE=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.pull(\"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.pull(\"nomic-embed-text\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from markdown import markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "VECTOR_DB_NAME = \"local-rag\"\n",
    "local_model = \"llama3.2\"\n",
    "llm = ChatOllama(model=local_model)\n",
    "vector_db = None\n",
    "\n",
    "# Function to load PDF\n",
    "def load_pdf(file_path):\n",
    "    loader = PyPDFLoader(file_path=file_path)\n",
    "    return loader.load()\n",
    "\n",
    "# Function to split text\n",
    "def split_text(data, chunk_size=1000, chunk_overlap=200):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return text_splitter.split_documents(data)\n",
    "\n",
    "# Function to create vector database\n",
    "def create_vector_db(chunks):\n",
    "    global vector_db\n",
    "    vector_db = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=OllamaEmbeddings(model=\"nomic-embed-text\"),\n",
    "        collection_name=VECTOR_DB_NAME,\n",
    "        persist_directory=\"./chroma_db\"  # Use a directory to persist embeddings\n",
    "#         persist_directory=None  # Set to None for in-memory storage\n",
    "    )\n",
    "    return \"Vector database created successfully\"\n",
    "\n",
    "# Function to set up retriever\n",
    "def get_retriever():\n",
    "    query_prompt = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"\"\"You are an AI language model assistant. Generate 2 different versions of the given user question to retrieve relevant documents from a vector database. Provide these alternative questions separated by newlines. Original question: {question}\"\"\",\n",
    "    )\n",
    "    return MultiQueryRetriever.from_llm(vector_db.as_retriever(), llm, prompt=query_prompt)\n",
    "\n",
    "# Function to create RAG chain\n",
    "def create_rag_chain():\n",
    "    retriever = get_retriever()\n",
    "    template = \"\"\"Answer the question based ONLY on the following context:\\n{context}\\nQuestion: {question}\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    return (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser()\n",
    "    )\n",
    "\n",
    "# Function to query the document\n",
    "def chat_with_pdf(question):\n",
    "    if not vector_db:\n",
    "        return \"Error: No vector database found. Please upload and process a PDF first.\"\n",
    "    chain = create_rag_chain()\n",
    "    response = chain.invoke(question)\n",
    "    return markdown(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This may be used in order to load the embeddigns from already created pdf file\n",
    "# # SO that we do not create the embeddings for the same file -> save processing\n",
    "# import hashlib\n",
    "\n",
    "# def get_document_id(file_path):\n",
    "#     \"\"\"Generate a unique document ID based on the file content hash.\"\"\"\n",
    "#     with open(file_path, \"rb\") as f:\n",
    "#         file_hash = hashlib.md5(f.read()).hexdigest()  # Create file hash\n",
    "#     return file_hash\n",
    "\n",
    "# def create_vector_db(chunks, file_path):\n",
    "#     global vector_db\n",
    "\n",
    "#     # Generate a unique document ID\n",
    "#     doc_id = get_document_id(file_path)\n",
    "\n",
    "#     # Check if the document already exists in the DB\n",
    "#     existing_docs = vector_db.get(ids=[doc_id])\n",
    "    \n",
    "#     if existing_docs and existing_docs[\"documents\"]:\n",
    "#         print(\"ðŸ“„ Document already exists. Loading from database...\")\n",
    "#         return vector_db  # Load existing embeddings\n",
    "\n",
    "#     print(\"âš¡ New document detected. Creating embeddings...\")\n",
    "\n",
    "#     # Store document with unique ID\n",
    "#     vector_db = Chroma.from_documents(\n",
    "#         documents=chunks,\n",
    "#         embedding=OllamaEmbeddings(model=\"nomic-embed-text\"),\n",
    "#         ids=[doc_id]  # Store document ID in the DB\n",
    "#     )\n",
    "#     return vector_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting posthog<3.0.0\n",
      "  Downloading posthog-2.5.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: requests<3.0,>=2.7 in /home/owais031/.local/lib/python3.8/site-packages (from posthog<3.0.0) (2.32.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/apps/anaconda3/2020.11/lib/python3.8/site-packages (from posthog<3.0.0) (1.16.0)\n",
      "Requirement already satisfied: monotonic>=1.5 in /home/owais031/.local/lib/python3.8/site-packages (from posthog<3.0.0) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /home/owais031/.local/lib/python3.8/site-packages (from posthog<3.0.0) (2.2.1)\n",
      "Requirement already satisfied: python-dateutil>2.1 in /opt/apps/anaconda3/2020.11/lib/python3.8/site-packages (from posthog<3.0.0) (2.8.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/apps/anaconda3/2020.11/lib/python3.8/site-packages (from requests<3.0,>=2.7->posthog<3.0.0) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/apps/anaconda3/2020.11/lib/python3.8/site-packages (from requests<3.0,>=2.7->posthog<3.0.0) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/owais031/.local/lib/python3.8/site-packages (from requests<3.0,>=2.7->posthog<3.0.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/apps/anaconda3/2020.11/lib/python3.8/site-packages (from requests<3.0,>=2.7->posthog<3.0.0) (2021.10.8)\n",
      "Downloading posthog-2.5.0-py2.py3-none-any.whl (36 kB)\n",
      "Installing collected packages: posthog\n",
      "  Attempting uninstall: posthog\n",
      "    Found existing installation: posthog 3.21.0\n",
      "    Uninstalling posthog-3.21.0:\n",
      "      Successfully uninstalled posthog-3.21.0\n",
      "Successfully installed posthog-2.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install \"posthog<3.0.0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF Loaded Successfully\n",
      "Text Split into Chunks\n",
      "Vector database created successfully\n",
      "Response: <p>This document appears to be a policy document related to education in India, specifically focusing on higher education. It outlines recommendations for reforming and revitalizing the country's higher education system, with an emphasis on quality, equity, and inclusion. The document discusses various issues facing the current system, such as fragmentation, lack of cognitive skills development, rigid specialization, and limited access, among others. It proposes a comprehensive set of changes to address these challenges and promote a high-quality, inclusive, and holistic higher education system that prepares students for meaningful lives and productive contributions to society.</p>\n"
     ]
    }
   ],
   "source": [
    "# Test the pipeline step by step\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"nep-6.pdf\"  # Change to the actual file path\n",
    "    \n",
    "    # Load PDF\n",
    "    data = load_pdf(pdf_path)\n",
    "    print(\"PDF Loaded Successfully\")\n",
    "    \n",
    "    # Split text\n",
    "    chunks = split_text(data[:2])\n",
    "    print(\"Text Split into Chunks\")\n",
    "    \n",
    "    # Create Vector DB\n",
    "    print(create_vector_db(chunks))\n",
    "    \n",
    "    # Test query\n",
    "    test_question = \"What is this document about?\"\n",
    "    print(\"Response:\", chat_with_pdf(test_question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: <p>Based on the provided context, here is a summary of the National Education Policy 2020 (NEP 2020):</p>\n",
      "<p>The policy aims to overhaul and re-energize the higher education system in India to overcome existing challenges. The key vision includes:</p>\n",
      "<ol>\n",
      "<li>Consolidating into large multidisciplinary universities and colleges with at least one in or near every district, offering medium of instruction or programs in local/Indian languages.</li>\n",
      "<li>Embracing a more multidisciplinary undergraduate education.</li>\n",
      "<li>Granting faculty and institutional autonomy.</li>\n",
      "<li>Revamping curriculum, pedagogy, assessment, and student support for enhanced student experiences.</li>\n",
      "<li>Reinforcing the integrity of faculty and institutional leadership positions through merit-based appointments and career progression based on teaching, research, and service.</li>\n",
      "<li>Establishing a National Research Foundation.</li>\n",
      "</ol>\n",
      "<p>The policy also highlights several challenges that need to be addressed, including:</p>\n",
      "<ol>\n",
      "<li>Fragmentation of higher education</li>\n",
      "<li>Limited emphasis on cognitive skills and learning outcomes</li>\n",
      "<li>Rigid separation of disciplines and early specialization</li>\n",
      "<li>Limited access, particularly in socio-economically disadvantaged areas</li>\n",
      "<li>Inadequate mechanisms for merit-based career management and progression</li>\n",
      "<li>Lesser emphasis on research and lack of competitive peer-reviewed funding</li>\n",
      "<li>Suboptimal governance and leadership</li>\n",
      "<li>An ineffective regulatory system</li>\n",
      "</ol>\n",
      "<p>The policy aims to deliver high-quality higher education with equity and inclusion, focusing on building vibrant communities of scholars and peers, promoting active research, and increasing resource efficiency across the higher education system.</p>\n"
     ]
    }
   ],
   "source": [
    "  # Test query\n",
    "test_question = \"Summarize NEP 2020?\"\n",
    "print(\"Response:\", chat_with_pdf(test_question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
