{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c52bbb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook uses mistral and embedding model from HF\n",
    "# Models are downloaded to the system so we should have enough RAM (min 16 GB) to run the script\n",
    "# We also require token from HF and sometime we need to accept the agreement on HF model that we are using e.g Mistral\n",
    "# After accepting we can use download that model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb7d57cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from markdown import markdown\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf7dacd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/owais031/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:128: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11080). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03546963b77a4282a1a1188d4f2d60f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load API keys from .env file\n",
    "load_dotenv()\n",
    "hf_api_key = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "\n",
    "# Define Hugging Face models\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # Small, fast embeddings\n",
    "LLM_MODEL = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "# Initialize Embeddings model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "\n",
    "# Load LLM as a text generation pipeline\n",
    "llm_pipeline = pipeline(\"text-generation\", model=LLM_MODEL, token=hf_api_key, max_new_tokens=200, use_fast=True)\n",
    "llm = HuggingFacePipeline(pipeline=llm_pipeline)\n",
    "\n",
    "# Global variables\n",
    "VECTOR_DB_NAME = \"huggingface-rag\"\n",
    "vector_db = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a46716a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a94a152a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load PDF\n",
    "def load_pdf(file_path):\n",
    "    loader = PyPDFLoader(file_path=file_path)\n",
    "    return loader.load()\n",
    "\n",
    "# Function to split text\n",
    "def split_text(data, chunk_size=1000, chunk_overlap=200):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return text_splitter.split_documents(data)\n",
    "\n",
    "# Function to create vector database\n",
    "def create_vector_db(chunks):\n",
    "    global vector_db\n",
    "    if not chunks:\n",
    "        return \"Error: No text extracted from the PDF.\"\n",
    "    \n",
    "    vector_db = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embedding_model,\n",
    "        collection_name=VECTOR_DB_NAME,\n",
    "        persist_directory=\"./chroma_db6\"\n",
    "#         persist_directory=None # Set to None for in-memory storage -> so that embeddings are not saved to disk\n",
    "    )\n",
    "    return \"PDF processed successfully! You can now ask questions.\"\n",
    "\n",
    "# Function to set up retriever\n",
    "def get_retriever():\n",
    "    if not vector_db:\n",
    "        return None\n",
    "    \n",
    "    query_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Generate 2 alternative versions of the question to improve retrieval:\n",
    "        Original: {question}\"\"\"\n",
    "    )\n",
    "    \n",
    "    return MultiQueryRetriever.from_llm(vector_db.as_retriever(), llm, prompt=query_prompt)\n",
    "\n",
    "# Function to create RAG chain\n",
    "def create_rag_chain():\n",
    "    retriever = get_retriever()\n",
    "    if not retriever:\n",
    "        return None\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"You are an AI assistant that answers questions based only on the given context.\n",
    "        Provide a well-structured, coherent, and concise response.\n",
    "\n",
    "        ### Context:\n",
    "        {context}\n",
    "\n",
    "        ### Question:\n",
    "        {question}\n",
    "\n",
    "        ### Answer:\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    return (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser()\n",
    "    )\n",
    "\n",
    "# Function to process user query\n",
    "def process_query(question, chat_history):\n",
    "    if not vector_db:\n",
    "        return \"Error: No vector database found. Please upload and process a PDF first.\", chat_history\n",
    "\n",
    "    chain = create_rag_chain()\n",
    "    if not chain:\n",
    "        return \"Error: Unable to initialize the RAG chain.\", chat_history\n",
    "\n",
    "    response = chain.invoke(question)\n",
    "    \n",
    "    # Extract only the answer from the response\n",
    "    answer_match = re.search(r\"### Answer:\\s*(.*)\", response, re.DOTALL)\n",
    "    answer = answer_match.group(1).strip() if answer_match else response.strip()\n",
    "    \n",
    "    chat_history.append((question, markdown(answer)))\n",
    "    return \"\", chat_history\n",
    "\n",
    "# Function to process PDF upload\n",
    "def process_pdf(file):\n",
    "    global vector_db\n",
    "    if not file:\n",
    "        return \"Please upload a valid PDF file.\"\n",
    "    \n",
    "    # Reset the vector DB before adding new embeddings\n",
    "    vector_db = None\n",
    "    # Load and process the PDF\n",
    "    file_path = file.name\n",
    "    data = load_pdf(file_path)\n",
    "    chunks = split_text(data)\n",
    "    \n",
    "    return create_vector_db(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8d014c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7866\n",
      "Running on public URL: https://6490426722ddecb799.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://6490426722ddecb799.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7866 <> https://6490426722ddecb799.gradio.live\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradio UI\n",
    "def gradio_ui():\n",
    "    with gr.Blocks(theme=\"soft\") as demo:\n",
    "        gr.Markdown(\"<h1 style='text-align: center; color: #4A90E2;'>📖 Conversational AI for PDFs</h1>\")\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=2):\n",
    "                chatbot = gr.Chatbot(label=\"AI Chat\")\n",
    "                user_input = gr.Textbox(placeholder=\"Ask me a question...\", label=\"Your Question\")\n",
    "\n",
    "                with gr.Row():\n",
    "                    ask_button = gr.Button(\"🔍 Ask\", variant=\"primary\")\n",
    "                    clear_button = gr.Button(\"🗑️ Clear\")\n",
    "\n",
    "            with gr.Column(scale=1):\n",
    "                gr.Markdown(\"### Upload PDFs Here:\")\n",
    "                pdf_upload = gr.File(label=\"Upload PDF\", file_types=[\".pdf\"])\n",
    "                status = gr.Textbox(label=\"Status\", interactive=False)\n",
    "\n",
    "        # Define button actions\n",
    "        ask_button.click(process_query, inputs=[user_input, chatbot], outputs=[user_input, chatbot])\n",
    "        clear_button.click(lambda: [], outputs=[chatbot])\n",
    "        pdf_upload.change(process_pdf, inputs=[pdf_upload], outputs=[status])\n",
    "\n",
    "    return demo\n",
    "\n",
    "demo = gradio_ui()\n",
    "demo.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8a673c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a30838",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rag_env)",
   "language": "python",
   "name": "rag_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
